# -*- encoding: utf8 -*-

# region LICENSE
# ------------------------------------------------------------------------------
# The MIT License
#
#
# Copyright 2023 Jorge A. Gomes
# 
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
# 
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
# 
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
# ------------------------------------------------------------------------------
# endregion (license)

# region IMPORTS

import sys

from typing import Any, Sequence

# endregion (imports)
# ---------------------------------------------------------
# region EXPORTS


__all__ = [
    'TPL_WARNING',
    'TPL_LICENSE',
    'TPL_DEPENDENCIES',
    'TPL_CONSTANTS',
    'TPL_GLOBALS',
    'TPL_UTILITIES',
    'TPL_API',
    'TPL_MAIN',
    'TPL_SOURCE_CLASS_1',
    'TPL_SOURCE_CLASS_2',
    'TPL_RELFILEPATH',
    'TPL_ABSFILEPATH',
    'TPL_RELDIRPATH',
    'TPL_ABSDIRPATH',
    'TPL_ENSURERELATIVE',
    'TPL_ENSUREABSOLUTE',
    'TPL_LOADANDPARSE',
]

# endregion (exports)
# ---------------------------------------------------------
# region CONSTANTS & ENUMS


TPL_WARNING = """# --------------------------------------------------------------------------------
# WARNING: DO NOT MODIFY THIS FILE! OR DO MODIFY, BUT BEFORE YOU PRECEED:

# * Understand that this file was generated by a parser generator.
# * Modifications may be overwritten by the generator.
# * If you own this file, you must know what you're doing.
# * Is not bad idea to have the grammar used to generate this source, in case you mess things up

# --------------------------------------------------------------------------------
"""

TPL_LICENSE = """# --------------------------------------------------------------------------------
# MIT License
#
# Copyright (c) {year} {cr_owner}
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:

# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.

# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
# --------------------------------------------------------------------------------
"""

TPL_DEPENDENCIES = """import re
import json
import time
import io
import os

from pathlib import Path
from argparse import ArgumentParser, Namespace, FileType
from colorama import just_fix_windows_console, Fore, Back, Style
from dataclasses import dataclass
from enum import IntEnum, auto
"""

TPL_GLOBALS = """
# the source code parsing abstraction
source = None

# stack of namespaces to use with push_scope(), pop_scope() and declare()
scope_stack = []
# verbosity level to use with push_verb(), pop_verb()
verbosity_stack = []
# helps to avoid pushing/popping scopes when testing rules
scopes_enabled = True
# The final AST: keys should be absolute filenames, values should be module nodes
abstract_syntax_tree = {}
# Token clasification
token_classifiers = []
# Token stream
token_stream = []
is_tokenizing = False
"""

TPL_CONSTANTS = """
class Verbosity(IntEnum):
    ERROR = 0
    WARNING = auto()
    DEBUG1 = auto()
    SUCCESS = auto()
    DEBUG2 = auto()
    INFO = auto()
    DEBUG3 = auto()
    ALL = auto()


ERROR = Verbosity.ERROR
WARNING = Verbosity.WARNING
DEBUG1 = Verbosity.DEBUG1
SUCCESS = Verbosity.SUCCESS
DEBUG2 = Verbosity.DEBUG2
INFO = Verbosity.INFO
DEBUG3 = Verbosity.DEBUG3
ALL = Verbosity.ALL

VERB_LEVELS = {
    'error': ERROR,
    'warning': WARNING,
    'debug1': DEBUG1,
    'success': SUCCESS,
    'debug2': DEBUG2,
    'info': INFO,
    'debug3': DEBUG3,
    'all': ALL
}

# region GENERATED CONSTANTS

# **RE** #

# endregion (generated constants)

"""

TPL_UTILITIES = """
def clsn(o: "Any") -> 'str':
    return o.__class__.__name__

def snakefy(string: str) -> 'str':
    lastchar = ''
    result = ''

    for char in string:
        if lastchar:
            if char.isupper():
                if lastchar.isupper() or lastchar.isdigit() or lastchar == '_':
                    result += f'{char.lower()}'
                else:
                    result += f'_{char.lower()}'
            else:
                result += char
        else:
            result += char.lower()
        lastchar = char

    return result

def is_token(*regexes):
    ""\"Returns True if source in current position matches the immediate token value, or False otherwise""\"
    global source
    for regex in regexes:
        if source.is_regex(regex):
            return True
    return False

def match_token(*regexes, token_classifier='any'):
    ""\"Tries to match an immediate token 'value' and returns its node, or None otherwise""\"
    global source
    location = source.location
    for regex in regexes:
        if m := source.match_regex(regex):
            log(False, debug3=f"Matched token with regex ```{ regex }``` at line {location[1]}, {location[2]}: '{m[0]}'")
            token = { 'kind': 'TOKEN', 'value': m[0], 'lc': [ location[1], location[2] ], 'classifier': classify(token_classifier) }
            grab_token(token, location)
            return token
    return None

def expect_token(*regexes, token_classifier='any'):
    \"""Demands the source in current position to match one of the given token values and returns its node or aborts with an error.""\"
    global source
    location = source.location
    for regex in regexes:
        if m := source.expect_regex(regex):
            token = { 'kind': 'TOKEN', 'value': m[0], 'lc': [ location[1], location[2] ], 'classifier': classify(token_classifier) }
            grab_token(token, location)
            return token
    return None

def log(localized=False, **messages):
    global source
    for level, message in messages.items():
        if level in VERB_LEVELS:
            verbosity = VERB_LEVELS.get(level)
        else:
            continue

        if verbosity <= source.verbosity:
            if verbosity is ERROR:
                source.error(message)
            elif verbosity is WARNING:
                source.warning(message)
            elif verbosity is DEBUG1:
                source.info(message, localized, True)
            elif verbosity is SUCCESS:
                source.success(message, localized)
            elif verbosity is DEBUG2:
                source.info(message, localized, True)
            elif verbosity is INFO:
                source.info(message, localized, False)
            elif verbosity is DEBUG3:
                source.info(message, localized, True)


def reduced(node, key):
    \"""Tries to reduce the node to its key""\"
    if isinstance(node, dict):
        if key in node and len(node) == 2:
            log(False, debug1=f"{node['kind']} reduced to its {key} item")
            return node[key]
        else:
            log(False, debug1=f"{node['kind']} not reduced")
    return node

def flipped(node, item_key, key):
    ""\"Tries to set the item as paret to the node\"""
    if isinstance(node, dict):
        if item := node.get(item_key):
            if isinstance(item, dict):
                del node[item_key]
                item[key] = node
                log(False, debug1=f"Item {item['kind']} now contains {node['kind']}")
                return item
            else:
                log(True, warning=f"node[{key}] is not a node (it's a {clsn(item)}, not a str)")
        else:
            log(False, debug1=f"{node['kind']} not flipped")
    log(False, debug2="No node to flip")
    return node

def merge(node, data, keep_kind = False):
    \"""Updates the node with all items in data""\"
    if isinstance(node, dict):
        if data:
            log(False, debug1=f"{node['kind']} merges with {data['kind']}")
            log(False, debug2=f"Before merge keys: {', '.join(node.keys())}")
            node.update(data)
            log(False, debug2=f"After merge keys: {', '.join(node.keys())}")
        else:
            log(True, debug1="Fail to merge node: data is None")
    else:
        source.warning("Fail to merge node: node is None")

def join(node, data):
    \"""Updates the node only with items in data that node does not have""\"
    if isinstance(node, dict):
        if data:
            log(False, debug1=f"{node['kind']} joins with {data['kind']}")
            log(False, debug2=f"Before join keys: {', '.join(node.keys())}")
            for key in data:
                if key not in node:
                    node[key] = data[key]
            log(False, debug2=f"After join keys: {', '.join(node.keys())}")
        else:
            log(True, debug1="Fail to join node: data is None")
    else:
        source.warning("Fail to join node: node is None")

def update(node, data, keep_kind = False):
    \"""Updates the node only with items in data that node does have""\"
    if isinstance(node, dict):
        if data:
            log(False, debug1=f"{node['kind']} joins with {data['kind']}")
            log(False, debug2=f"Before join keys: {', '.join(node.keys())}")
            for key in data:
                if key in node:
                    node[key] = data[key]
            log(False, debug2=f"After join keys: {', '.join(node.keys())}")
        else:
            log(True, debug1="Fail to join node: data is None")
    else:
        source.warning("Fail to join node: node is None")

def scope_lookup(name, must_find = False):
    \"""Searches for the given name in the scope chain and returns its reference""\"
    global scope_stack

    ref = { 'ref_kind': 'NOT_FOUND', 'ref_name': name, 'inside_scope': None }

    if len(scope_stack):
        for scope in reversed(scope_stack):
            if node := scope.get(name):
                return { 'ref_kind': node['kind'], 'ref_name': name, 'inside_scope': scope['~[kind]~'] }

    if must_find:
        source.error(f"Name not found: '{name}'")

    return ref

def node_lookup(node, member, kind):
    \"""Extracts the member item from the node""\"
    if isinstance(node, dict):
        if m := node.get(member):
            log(False, debug1=f"{node['kind']}['{member}'] extracted")
            return m
        source.error(f"{kind} node has no '{member}' item")
    source.warning(f"Fail to lookup '{member}' item: {kind} node is None")

def push_verb(verbosity_level, do_log=False):
    global verbosity_stack, source

    if verbosity_level in VERB_LEVELS:
        if do_log:
            source.info(f"VERBOSITY: {source.verbosity.name.lower()} -> {verbosity_level}", False, False)

        verbosity_stack.append(source.verbosity)
        source.verbosity = VERB_LEVELS.get(verbosity_level)

def pop_verb(do_log=False):
    global verbosity_stack, source

    if len(verbosity_stack):
        if do_log:
            source.info(f"VERBOSITY: {verbosity_stack[-1].name.lower()} <- {source.verbosity.name.lower()}", False, False)

        source.verbosity = verbosity_stack.pop()

def push_scope(just_checking=False, kind='scope'):
    \"""Adds namespace behavior to the current node""\"
    global scope_stack, scopes_enabled

    if not just_checking:
        parent_scope = scope_stack[-1] if len(scope_stack) else None

        scope = {
            '~[kind]~': kind
            '~[parent]~': parent_scope.get('~[kind]~')
        }

        scope_stack.append({ '$cope_kind': None })

def pop_scope(node, name, just_checking=False):
    \"""Assigns the current scope to the onwer node""\"
    global scope_stack, scopes_enabled

    if not just_checking:
        if len(scope_stack):
            scope = scope_stack.pop()

            if isinstance(node, dict):
                node[name] = scope
                log(False, debug1=f"Scope assigned into node {node['kind']} under '{name}' key")
                log(False, debug2=f"Scope has keys: {', '.join(scope.keys())}")
                log(False, debug2=f"The {node['kind']} node has keys: {', '.join(node.keys())}")

            else:
                source.warning("No node to assign scope into")
        else:
            source.error("No scope to pop")

def append(node, key, value):
    \"""Appends the value to the list in node[key]""\"
    if isinstance(node, dict):
        items = node.get(key, [])
        if isinstance(items, list):
            items.append(value)
            node[key] = items
        else:
            log(True, warning=f"Cannot append to {key} in {node['kind']} node: not a list")
    else:
        log(True, warning=f"No node with '{key}' to append value into")

def declare(identifier_key, node, kind):
    \"""Declares a node in the current scope""\"
    global scope_stack, scopes_enabled
    # if scopes_enabled:
    if len(scope_stack):
        scope = scope_stack[-1]

        if not node:
            source.warning(f"Fail to declare {kind} node: None")
            return

        if not isinstance(identifier_key, str):
            source.error("Invalid identifier_key type")

        if identifier_key not in node:
            source.warning(f"Failed to declare {kind} node: missing identifier key '{identifier_key}'")
            return

        if name := node.get(identifier_key):
            if not isinstance(name, str):
                source.error(f"Invalid node identifier type (it's a {clsn(name)}, not a str)")

            if name in scope:
                source.error(f"Redeclared name in scope: {name}")

            scope[name] = node
            log(False, debug1=f"{node['kind']} declared")
            log(False, success=f"{node['kind']} declared in scope as {name}, having keys {', '.join(node.keys())}")
        else:
            source.error(f"{node['kind']} has no {identifier_key} item")
    else:
        source.error("No active scope")

def push_classifier(classifier):
    \"""Pushes a token classifier""\"
    global token_classifiers
    token_classifiers.append(classifier)

def pop_classifier():
    \"""Pops a token classifier""\"
    global token_classifiers
    token_classifiers.pop()

def unload_classifiers():
    \"""Pops all token classifiers""\"
    global token_classifiers
    clfrs = token_classifiers[:]
    token_classifiers.clear()
    return clfrs

def load_classifiers(clfrs, clear_first = True):
    \"""Pushes a sequence of token classifiers""\"
    global token_classifiers
    if clear_first:
        token_classifiers.clear()
    token_classifiers.extend(clfrs)

def retroclassify(new_clfr):
    global token_stream, token_classifiers

    curr_clfr = '.'.join(token_classifiers)
    i = len(token_stream) -1

    while i >= 0:
        tkn_clfr = token_stream[i].get('classifier')

        if tkn_clfr.startswith(curr_clfr):
            token_stream[i]['classifier'] = token_stream[i]['classifier'].replace(curr_clfr, new_clfr)

        else:
            break
        i -= 1

    token_classifiers.clear()
    token_classifiers.extend(new_clfr.split('.'))


def classify(token_kind):
    \"""Returns the fully qualified token classification""\"
    global token_classifiers
    clfr = '.'.join(token_classifiers)
    if clfr and token_kind:
        return f"{'.'.join(token_classifiers)}.{token_kind}"
    elif clfr:
        return clfr
    elif token_kind:
        return token_kind
    else:
        return "other"

def grab_token(token, location):
    global token_stream

    fname, lin, col, index, line = location
    length = len(token.get('value', ''))
    tkn = { 'start_index': index, 'end_index': index + length, 'length': length, 'line': lin, 'column': col, 'classifier': token.get('classifier', '<unknown>') }
    token_stream.append(tkn)
"""


TPL_RELFILEPATH = """if os.path.isabs(m_path) or os.path.isdir(m_path) or not os.path.exists(m_path):
    m_path_valid = False
    m_path_message = f"Expected an existing relative file path"
    m_path_error = True
"""

TPL_ABSFILEPATH = """if not os.path.isabs(m_path) or os.path.isdir(m_path) or not os.path.exists(m_path):
    m_path_valid = False
    m_path_message = f"Expected an existing absolute file path"
    m_path_error = True
"""

TPL_RELDIRPATH = """if os.path.isabs(m_path) or os.path.isfile(m_path) or not os.path.exists(m_path):
    m_path_valid = False
    m_path_message = f"Expected an existing relative directory path"
    m_path_error = True
"""

TPL_ABSDIRPATH = """if not os.path.isabs(m_path) or os.path.isfile(m_path) or not os.path.exists(m_path):
    m_path_valid = False
    m_path_message = f"Expected an existing absolute directory path"
    m_path_error = True
"""

TPL_ENSURERELATIVE = """if os.path.isabs(m_path):
    try:
        m_path = os.path.relpath(os.getcwd())
    except ValueError:
        m_path_valid = False
        m_path_message = "Path is not a subpath of current working directory"
        m_path_error = True
"""

TPL_ENSUREABSOLUTE = """if not os.path.isabs(m_path):
    try:
        m_path = os.path.abspath(m_path)
    except RuntimeError:
        m_path_valid = False
        m_path_message = "Unable to resolve path as absolute"
        m_path_error = True
"""

TPL_LOADANDPARSE = """if os.path.isfile(m_path) and os.path.exists(m_path):
    spath = os.path.abspath(os.path.normpath(m_path))
    try:
        parse(spath)
    except Exception as e:
        log(True, error=f"Unable to parse file: {', '.join(str(arg) for arg in e.args)}")
"""


TPL_API = """
def parse(source_fname, start_rule='{start_rule}', verbosity='error'):
    ""\"Parsers a source file and generates an abstract syntax tree of the source.\"""
    global source

    saved_cwd: str = os.getcwd()
    cwd_path = os.path.dirname(source_fname)
    cwf_path = os.path.basename(source_fname)
    os.chdir(cwd_path)
    abspath = os.path.abspath(os.path.normpath(cwf_path))

    if abspath not in abstract_syntax_tree:

        abstract_syntax_tree[abspath] = {{ 'kind': 'CURRENTLY_PARSING' }}

        message = f"Started parsing file {{source_fname}}"
        header = f"{{Fore.BLACK}}{{Back.CYAN}}INFO: {{Style.BRIGHT}}{{Fore.CYAN}}{{Back.BLACK}} {{message}}{{Style.RESET_ALL}}"
        print(header)

        main_rule = f'match_{{snakefy(start_rule)}}'
        rule_callback = globals().get(main_rule, lambda: {{ 'kind': 'NO_AST', 'message': f'{start_rule} not defined' }})

        with open(cwf_path, 'r', encoding='utf8') as fp:
            source_contents = fp.read()

        saved_source = source
        kept_verb: str = saved_source.verbosity.name.lower() if saved_source else verbosity
        source = Source(source_contents, source_contents, source_fname, VERB_LEVELS.get(kept_verb, ERROR))
        source.skip()

        ptime = time.process_time()
        ast = rule_callback()
        delta = time.process_time() - ptime

        abstract_syntax_tree[abspath] = ast

        source = saved_source
        os.chdir(saved_cwd)

        message = f"Finished parsing file {{source_fname}} (took {{delta:.4f}} seconds)"
        header = f"{{Fore.BLACK}}{{Back.CYAN}}INFO: {{Style.BRIGHT}}{{Fore.CYAN}}{{Back.BLACK}} {{message}}{{Style.RESET_ALL}}"
        print(header)

    else:
        ast = abstract_syntax_tree.get(abspath)
        if ast.get('kind', '') == 'CURRENTLY_PARSING':
            message = f"Cyclic reference detected in {{source_fname}}"
            header = f"{{Fore.BLACK}}{{Back.YELLOW}}WARNING: {{Style.BRIGHT}}{{Fore.YELLOW}}{{Back.BLACK}} {{message}}{{Style.RESET_ALL}}"
            print(header)

        message = f"File {{source_fname}} was previously parsed. Skipping..."
        header = f"{{Fore.BLACK}}{{Back.CYAN}}INFO: {{Style.BRIGHT}}{{Fore.CYAN}}{{Back.BLACK}} {{message}}{{Style.RESET_ALL}}"
        print(header)

def generate_ast(source_fname, start_rule='{start_rule}', verbosity='error'):
    \"""Parses the source file and returns its AST""\"
    global abstract_syntax_tree
    parse(source_fname, start_rule, verbosity)

    return abstract_syntax_tree

def generate_stream(source_fname, start_rule='{start_rule}', verbosity='error'):
    \"""Parses the source file and returns its stream of tokens""\"
    global token_stream, is_tokenizing

    is_tokenizing = True
    try:
        parse(source_fname, start_rule, verbosity)
    except GrammarTokenizerStop:
        pass

    return token_stream
"""

TPL_MAIN = """just_fix_windows_console()

parser = ArgumentParser()
parser.add_argument('source', help="The source file path", type=FileType('r', encoding='utf8'))
parser.add_argument('-o', '--out', help="The AST output file path", type=FileType('w', encoding='utf8'), required=True)
parser.add_argument('-s', '--start', help="Set starting rule to parse", type=str, default='{start_rule}')
parser.add_argument('-v', '--verbosity', help="Set the verbosity level", choices=['error', 'warning', 'debug1', 'success', 'debug2', 'info', 'debug3', 'all'], default='error')
parser.add_argument('-t', '--tokenize', help="Generates a stream of tokens instead of an AST", action='store_true')

args: Namespace = parser.parse_args()

abspath = os.path.abspath(os.path.normpath(args.source.name))
parse(abspath, args.start, args.verbosity)

if args.tokenize:
    try:
        json.dump(token_stream, args.out, indent=2)
        done = True
    except Exception as e:
        done = False
        message = f"Failed to save token stream into file: {{' '.join(repr(arg) for arg in e.args)}}"
        header = f"{{Fore.BLACK}}{{Back.RED}}ERROR: {{Style.BRIGHT}}{{Fore.RED}}{{Back.BLACK}} {{message}}{{Style.RESET_ALL}}"
        print(header)

    if done:
        message = f"Saved Token stream into file: '{{args.out.name}}'"
        header = f"{{Fore.BLACK}}{{Back.GREEN}}SUCCESS: {{Style.BRIGHT}}{{Fore.GREEN}}{{Back.BLACK}} {{message}}{{Style.RESET_ALL}}"
        print(header)

elif abstract_syntax_tree:
    try:
        json.dump(abstract_syntax_tree, args.out, indent=2)
        done = True
    except Exception as e:
        done = False
        message = f"Failed to save ast into file: {{' '.join(repr(arg) for arg in e.args)}}"
        header = f"{{Fore.BLACK}}{{Back.RED}}ERROR: {{Style.BRIGHT}}{{Fore.RED}}{{Back.BLACK}} {{message}}{{Style.RESET_ALL}}"
        print(header)

    if done:
        message = f"Saved AST into file: '{{args.out.name}}'"
        header = f"{{Fore.BLACK}}{{Back.GREEN}}SUCCESS: {{Style.BRIGHT}}{{Fore.GREEN}}{{Back.BLACK}} {{message}}{{Style.RESET_ALL}}"
        print(header)
else:
    message = "Done without significative output"
    header = f"{{Fore.BLACK}}{{Back.YELLOW}}WARNING: {{Style.BRIGHT}}{{Fore.YELLOW}}{{Back.BLACK}} {{message}}{{Style.RESET_ALL}}"
    print(header)

return 0
"""

TPL_SOURCE_CLASS_1 = """

class GrammarParserError(Exception):
    pass


class GrammarTokenizerStop(Exception):
    pass


@dataclass
class Source:
    contents: str
    current: str
    filename: str
    verbosity: Verbosity = ERROR

    @property
    def location(self) -> 'tuple[str, int, int, int, str]':
        ""\"Returns a 4-tuple containing the filename, line number, column, and line of code\"""
        consumed = len(self.contents) - len(self.current)
        consumed_lines = self.contents[0: consumed].split('\\n')
        line_num = len(consumed_lines)
        col_num = len(consumed_lines[-1]) + 1
        remaining_line = self.current.split('\\n')[0]
        line = f"  {consumed_lines[-1]}{remaining_line}"

        return self.filename, line_num, col_num, consumed, line

    @property
    def index(self) -> 'int':
        ""\"Gets or sets the current string index of the grammar contents\"""
        return len(self.contents) - len(self.current)

    @index.setter
    def index(self, value: int) -> 'None':
        ind = max(0, min(value, len(self.contents) - 1))
        self.current = self.contents[ind:]

    def skip(self):
        ""\"Skip over whitespace and comments\"""
        while True:"""

TPL_SOURCE_CLASS_2 = """
            break

    def expect_regex(self, regex: 'str | re.Pattern', error_message: str | None = None) -> 're.Match':
        ""\"Tries to match regex and returns its match object. Prints an error otherwise.\"""
        if m := self.match_regex(regex):
            return m
        self.error(error_message or f"Expected '{regex}'")

    def match_regex(self, regex: 'str | re.Pattern', advance: bool = True, skip: bool = True) -> 're.Match | Bool | None':
        ""\"Tries to match a regex, optionally consumes it and returns Match/None, otherwise a True/False.\"""
        m: 're.Match | None' = re.match(regex, self.current) if isinstance(regex, str) else regex.match(self.current)
        if m:
            if advance:
                self.current = self.current[len(m[0]):]
                if self.verbosity >= DEBUG3:
                    self.info(f"Match success: {repr(regex)}", as_debug=True)
                if skip:
                    self.skip()
            return m

        if advance and self.verbosity >= DEBUG3:
            self.info(f"Match fail: {repr(regex)}", as_debug=True)
        return None

    def is_regex(self, regex: 'str | re.Pattern') -> 'bool':
        ""\"Returns whether a regex matches.\"""
        if re.match(regex, self.current):
            return True
        return False

    def _log(self, severity: str, message: str, localized: bool = True, at_index: int | None = None, color='WHITE', sys_exit: bool = False, raise_exc: bool = False, out_file: io.IOBase = sys.stdout) -> 'None':
        ""\"Prints an log message.""\"
        global is_tokenizing
        saved_index = self.index
        if isinstance(at_index, int):
            self.index = at_index
        file, lin, col, index, line = self.location
        self.index = saved_index

        header = f"{Fore.BLACK}{getattr(Back, color, Back.WHITE)}{severity}: {Style.BRIGHT}{getattr(Fore, color, Fore.WHITE)}{Back.BLACK} {message}{Style.RESET_ALL}"
        location = f"{file}:{lin}:{col}{Style.RESET_ALL}"
        pointer = f"  {' ' * (col - 1)}{getattr(Fore, color, Fore.WHITE)}^{Style.RESET_ALL}"

        if raise_exc:
                raise GrammarParserError(message)
        else:
            if localized:
                print(f"{header}\\n{location}\\n{line}\\n{pointer}", file=out_file)
            else:
                print(header, file=out_file)
            
            if sys_exit:
                if is_tokenizing:
                    raise GrammarTokenizerStop(message)
                else:
                    sys.exit(1)

    def error(self, message: str, at: int | None = None) -> 'None':
        ""\"Aborts with an error message.\"""
        self._log('ERROR', message, at_index=at, color='RED', sys_exit=True, out_file=sys.stderr)

    def warning(self, message: str, at: int | None = None) -> 'None':
        ""\"Aborts with an error message.""\"
        self._log('WARNING', message, at_index=at, color='YELLOW')

    def info(self, message: str, localized: bool = True, as_debug: bool = False) -> 'None':
        ""\"Prints an information message.""\"
        self._log('DEBUG' if as_debug else 'INFO', message, localized, color='WHITE' if as_debug else 'CYAN')

    def success(self, message: str, localized: bool = True) -> 'None':
        ""\"Prints an success message.\"""
        self._log('SUCCESS', message, localized, color='GREEN')

"""

# endregion (constants)
